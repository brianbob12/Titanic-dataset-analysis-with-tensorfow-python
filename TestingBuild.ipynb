{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingBuild.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaP3hHEP_kSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuHfp4DmzzlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(x):#takes a row of the dataset and preps it for nural net\n",
        "  out=[]\n",
        "  y=x[0]\n",
        "  x=x[1:]\n",
        "  x=np.array(x)\n",
        "  for i,v in enumerate(x):\n",
        "    if i!=7 and i!=8:\n",
        "      x[i]=np.array(v)\n",
        "  for i in x:\n",
        "    try:\n",
        "      for j in i:\n",
        "        out.append(float(j))\n",
        "    except:\n",
        "      out.append(float(i))\n",
        "  \n",
        "  return out,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5t5r8w8UKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def liniar1(arg):\n",
        "  return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRJl7j1FX1TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_url=\"https://raw.githubusercontent.com/brianbob12/brianbob12.github.io/master/titanic_train.csv\"\n",
        "train_data=pd.read_csv(train_url)\n",
        "train_data=train_data.drop(columns=[\"Name\",\"Ticket\"])#remove irrelecent or unusable data\n",
        "\n",
        "#split the cabin data into two poits\n",
        "pre_CabinA=[]#CabinA is the cabin letter(as an array of 8  -Nan and the 7 letters)\n",
        "#pre_CabinA    [A,B,C,D,E,F,G,T,nan]\n",
        "pre_CabinB=[]#CabinB is the cabin number 0 for nan\n",
        "for cab in train_data[\"Cabin\"]:\n",
        "  if str(cab) ==\"nan\":\n",
        "    pre_CabinA.append(np.array([0 for i in range(8)]))\n",
        "    pre_CabinB.append(0)\n",
        "  elif len(str(cab).split(\" \"))>1:\n",
        "    a=str(cab).split(\" \")\n",
        "    for pos in a:\n",
        "      if len(str(pos))>1:\n",
        "        new_cab=pos\n",
        "    \n",
        "    let=new_cab[0]\n",
        "    to_let=[0 for i in range(8)]\n",
        "    if let!=\"T\":\n",
        "      to_let[ord(let)-65]=1\n",
        "    else:\n",
        "      to_let[6]=1\n",
        "    num=int(new_cab[1:])\n",
        "    pre_CabinA.append(np.array(to_let))\n",
        "    pre_CabinB.append(num)\n",
        "  else:\n",
        "    \n",
        "    if len(str(cab))==1:\n",
        "      new_cab=cab+\"0\"\n",
        "    else:\n",
        "      new_cab=cab\n",
        "    \n",
        "    let=new_cab[0]\n",
        "    to_let=[0 for i in range(8)]\n",
        "    if let!=\"T\":\n",
        "      to_let[ord(let)-65]=1\n",
        "    else:\n",
        "      to_let[6]=1\n",
        "    num=int(new_cab[1:])\n",
        "    pre_CabinA.append(np.array(to_let))\n",
        "    pre_CabinB.append(num)\n",
        "train_data[\"CabinA\"]=pre_CabinA\n",
        "train_data[\"CabinB\"]=pre_CabinB\n",
        "new_age=[]\n",
        "for ind in train_data[\"Age\"]:\n",
        "  if str(ind).lower()==\"nan\":\n",
        "    new_age.append(-1)#unavailable age\n",
        "  else:\n",
        "    new_age.append(int(ind))\n",
        "train_data[\"Age\"]=new_age\n",
        "train_data=train_data.drop(columns=[\"Cabin\",\"PassengerId\"])\n",
        "\n",
        "#embarked\n",
        "#array of 4\n",
        "new_embark=[]\n",
        "un=train_data[\"Embarked\"].unique()\n",
        "for i in train_data[\"Embarked\"]:\n",
        "  for inx,j in enumerate(un):\n",
        "    if str(i)==str(j):\n",
        "      ps=[0 for i in range(4)]\n",
        "      ps[inx]=1\n",
        "      new_embark.append(np.array(ps))\n",
        "      break\n",
        "train_data[\"Embarked\"]=new_embark\n",
        "#sex\n",
        "#0-female\n",
        "#1-male\n",
        "new_sex=[]\n",
        "for i in train_data[\"Sex\"]:\n",
        "  if i==\"female\":\n",
        "    new_sex.append(0)\n",
        "  else:\n",
        "    new_sex.append(1)\n",
        "train_data[\"Sex\"]=new_sex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-xICg_YO4Jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TESTING PROTOCOL\n",
        "n_test=30\n",
        "test_const=5#test constant spreads out the testdata amung the dataset\n",
        "hold_out=[]#the hold_out data is a lit of indeces for traking progress of the network\n",
        "for i in range(n_test):\n",
        "  hold_out.append((i*test_const)%(891+1))\n",
        "\n",
        "testing_log=[[],[],[],[]]#testing_log[0] is ,iteration testing_log[1] is accurasy testting_log[2] is average guess testing_log[3] is train error\n",
        "\n",
        "def Test(iteration):\n",
        "  batch_X=[]\n",
        "  batch_Y=[]\n",
        "  for indx in hold_out:\n",
        "    x,y=prep(train_data.loc[indx])\n",
        "    batch_X.append(x)\n",
        "    batch_Y.append([y])\n",
        "  error=math.sqrt(sess.run(tf.reduce_mean(cross_entropy),feed_dict={X:batch_X,Y:batch_Y,dropout:0}))\n",
        "  average_guess=tf.math.reduce_mean(sess.run(out_layer,feed_dict={X:batch_X,Y:batch_Y,dropout:0}))\n",
        "  print(\"-\"*15)\n",
        "  print(\"iteration:\",iteration)\n",
        "  print(\"average guess:\",sess.run(average_guess))\n",
        "  print(\"error:\",error)\n",
        "  print(\"-\"*15)\n",
        "  testing_log[0].append(iteration)\n",
        "  testing_log[1].append(error)\n",
        "  testing_log[2].append(sess.run(average_guess))\n",
        "  \n",
        "ans=[]\n",
        "for indx in hold_out:\n",
        "    x,y=prep(train_data.loc[indx]) \n",
        "    ans.append(y)\n",
        "test_data_average=sum(ans)/len(ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xv_qeihi7YU",
        "colab_type": "code",
        "outputId": "02344542-5015-40bc-bebe-b79917d5f8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "surviv_rate=sum(train_data[\"Survived\"])/train_data.count()[\"Survived\"]\n",
        "print(\"data survival rate:\"+str(surviv_rate))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data survival rate:0.3838383838383838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Turr5TVdB25_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTjrbCmN8G5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y=prep(train_data.loc[1])\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPf_kBhp6gfV",
        "colab_type": "text"
      },
      "source": [
        "network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1MGXqjtX_ZE",
        "colab_type": "code",
        "outputId": "cf610fca-dd06-4f67-8d4c-9e7bcf841139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#CONSTANTS\n",
        "n_input=19\n",
        "n_out=1\n",
        "n_layers=4#excludes input layer includes output layer\n",
        "layer_number=[128,64,32,1]#number of neurons in each layer, excludes input layer includes output layer\n",
        "layer_activation=[tf.nn.relu,tf.nn.relu,tf.nn.relu,tf.nn.relu,tf.nn.relu]#activation function for each layer\n",
        "training_keep_probs=[0.5,0.5,1]#for each layer REMEMBER SET OUTPUT KEEP PROB TO 1\n",
        "#for the output 0=died 1=survived\n",
        "\n",
        "learning_rate=0.05\n",
        "batch_size=train_data.count()[0]-n_test#the whole dataset\n",
        "batch_iterations=180\n",
        "total_iterations=batch_size*batch_iterations\n",
        "#if total_iterations>train_data.shape[0]:\n",
        "#  print(\"only\",train_data.shape[0],\"datapoints available\")\n",
        "#start_weight=0.01\n",
        "#weight_constant=0.001\n",
        "\n",
        "X=tf.placeholder(\"float\",[None,n_input])\n",
        "Y=tf.placeholder(\"float\",[None,n_out])\n",
        "dropout=tf.placeholder(\"float\")#if 1 then dropout occurs ,if 0 no dropout occurs\n",
        "\n",
        "test_occurance=6#in batch iterations\n",
        "\n",
        "print(\"current structure:\")\n",
        "print(\"learning rate:\\t\"+str(learning_rate))\n",
        "print(\"batch size:\\t\"+str(batch_size))\n",
        "print(\"\\tinput layer positive float\\t-19-\")\n",
        "for i in range(n_layers-1):\n",
        "  print(\"\\thidden layer positive float\\t\",end=\"\")\n",
        "  print(\"-\"+str(layer_number[i])+\"-\\tactivation:\"+str(layer_activation[i]))\n",
        "print(\"\\toutput layer positive float\\t-1-\\tactivation:\"+str(layer_activation[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current structure:\n",
            "learning rate:\t0.05\n",
            "batch size:\t861\n",
            "\tinput layer positive float\t-19-\n",
            "\thidden layer positive float\t-128-\tactivation:<function relu at 0x7f0ea59dde18>\n",
            "\thidden layer positive float\t-64-\tactivation:<function relu at 0x7f0ea59dde18>\n",
            "\thidden layer positive float\t-32-\tactivation:<function relu at 0x7f0ea59dde18>\n",
            "\toutput layer positive float\t-1-\tactivation:<function relu at 0x7f0ea59dde18>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxFOAVC_wDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights=[]\n",
        "biases=[]\n",
        "layers=[]\n",
        "dropout_layers=[]\n",
        "\n",
        "for i in range(n_layers):\n",
        "  biases.append(tf.Variable(tf.constant(0.1,shape=[layer_number[i]])))\n",
        "  if i==0:#first layer\n",
        "    #weights.append(tf.Variable(tf.constant(start_weight,shape=[n_input,layer_number[i]])))\n",
        "    weights.append(tf.Variable(\n",
        "        tf.math.scalar_mul(tf.math.sqrt(1/layer_number[i]**(i)),tf.truncated_normal([n_input,layer_number[i]],stddev=0.1))\n",
        "                  ))\n",
        "    #layers.append(tf.nn.dropout(layer_activation[i](tf.math.add(tf.matmul(X,weights[i]),biases[i])),rate=1-(training_keep_probs[i]*dropout)))\n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(X,weights[i]),biases[i])))\n",
        "    \n",
        "  else:\n",
        "    #weights.append(tf.Variable(tf.constant(start_weight+,shape=[layer_number[i-1],layer_number[i]])))\n",
        "    weights.append(tf.Variable(\n",
        "        tf.math.scalar_mul(tf.math.sqrt(1/layer_number[i]**(i)),tf.truncated_normal([layer_number[i-1],layer_number[i]],stddev=0.1))\n",
        "                  ))\n",
        "    #layers.append(tf.nn.dropout(layer_activation[i](tf.math.add(tf.matmul(layers[-1],weights[i]),biases[i])),rate=1-(training_keep_probs[i]*dropout)))\n",
        "    layers.append(layer_activation[i](tf.math.add(tf.matmul(layers[i-1],weights[i]),biases[i])))\n",
        "\n",
        "out_layer=layers[-1]#take the final layer\n",
        "\n",
        "#cross_entropy=tf.reduce_mean(tf.nn.sigmid_cross_entropy_with_logits(labels=Y,logits=out_layer))\n",
        "cross_entropy=tf.reduce_mean(tf.square(tf.math.subtract(out_layer,Y)))\n",
        "training_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2RGXTqK6vOM",
        "colab_type": "code",
        "outputId": "c443d598-e09c-4b74-81b3-cee0d4adea76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1253
        }
      },
      "source": [
        "testing_log=[[],[],[],[]]\n",
        "init=tf.global_variables_initializer()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "Test(0)\n",
        "place=0#excludes test data\n",
        "for i in range(batch_iterations):\n",
        "  batch_X=[]\n",
        "  batch_Y=[]\n",
        "  for j in range(batch_size):\n",
        "    x,y=prep(train_data.loc[place])\n",
        "    place+=1\n",
        "    place%=train_data.count()[0]\n",
        "    while place in hold_out:#if the index in test_data just keep going\n",
        "      place+=1\n",
        "      place%=train_data.count()[0]\n",
        "    batch_X.append(x)\n",
        "    batch_Y.append([y])\n",
        "  results=sess.run([cross_entropy,training_step],feed_dict={X:batch_X,Y:batch_Y,dropout:1})\n",
        "  if (i+1)%test_occurance==0:\n",
        "    x=math.sqrt(results[0])\n",
        "    print(\"training error:\",x)\n",
        "    testing_log[3].append(x)\n",
        "    Test(i+1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------\n",
            "iteration: 0\n",
            "average guess: 0.12642089\n",
            "error: 0.43645913762069316\n",
            "---------------\n",
            "training error: 0.5759272945284326\n",
            "---------------\n",
            "iteration: 6\n",
            "average guess: 0.34318197\n",
            "error: 0.6571107850259763\n",
            "---------------\n",
            "training error: 0.6237647485996105\n",
            "---------------\n",
            "iteration: 12\n",
            "average guess: 0.0\n",
            "error: 0.48304589256792574\n",
            "---------------\n",
            "training error: 0.6237647485996105\n",
            "---------------\n",
            "iteration: 18\n",
            "average guess: 0.0\n",
            "error: 0.48304589256792574\n",
            "---------------\n",
            "training error: 0.6237647485996105\n",
            "---------------\n",
            "iteration: 24\n",
            "average guess: 0.0\n",
            "error: 0.48304589256792574\n",
            "---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d67b2b69591e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplace\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplace\u001b[0m\u001b[0;34m%=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhold_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#if the index in test_data just keep going\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mplace\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, axis, level, numeric_only)\u001b[0m\n\u001b[1;32m   5643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5645\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5646\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5647\u001b[0m                 \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   7293\u001b[0m                                       skipna=skipna, min_count=min_count)\n\u001b[1;32m   7294\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 7295\u001b[0;31m                             numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m   7296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7297\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   5693\u001b[0m     def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n\u001b[1;32m   5694\u001b[0m                 filter_type=None, **kwds):\n\u001b[0;32m-> 5695\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5697\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_ALIASES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZG6SpPxbQDL",
        "colab_type": "code",
        "outputId": "d00f6f7b-c873-4fb4-b0d8-93f840259632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "plt.plot(testing_log[0],testing_log[1])#graphing error\n",
        "plt.plot(testing_log[0],testing_log[2])#graphing average guess\n",
        "plt.plot([0,0],[0,1],\"r\")\n",
        "plt.plot([0,batch_iterations],[0.5,0.5],\"r\")\n",
        "plt.plot([0,batch_iterations],[test_data_average,test_data_average],\"y\")\n",
        "plt.plot(testing_log[0][1:],testing_log[3])\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKvg9CZi6g5W",
        "colab_type": "code",
        "outputId": "34f66a73-6ca6-4d87-9d6e-5b429fc672ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "sess.run(weights[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.16443181, -0.10576075, -0.05585624, ...,  0.10315685,\n",
              "        -0.02623703,  0.00111963],\n",
              "       [-0.20860457,  0.00634965,  0.08074883, ..., -0.05489729,\n",
              "         0.03758308, -0.10141505],\n",
              "       [-0.09730501, -0.04462957, -0.00532899, ..., -0.05176582,\n",
              "         0.01044383, -0.1996431 ],\n",
              "       ...,\n",
              "       [-0.1788281 , -0.11392321, -0.13892026, ...,  0.01962324,\n",
              "        -0.07983497, -0.23447305],\n",
              "       [ 0.09623888, -0.08428939,  0.08082709, ...,  0.11812884,\n",
              "         0.13036336,  0.02489224],\n",
              "       [-0.03439557, -0.00041214, -0.00092972, ..., -0.05083841,\n",
              "         0.04666192, -0.00682013]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUyRH-cvCOjc",
        "colab_type": "code",
        "outputId": "583178d8-3415-4edc-fb05-d9aafdfe0458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "test_i=861\n",
        "print(train_data.loc[test_i])\n",
        "x,y=prep(train_data.loc[test_i])\n",
        "test_guess = sess.run(out_layer, feed_dict={X:[x], Y: [[y]],dropout:0})\n",
        "error=sess.run(cross_entropy,feed_dict={X:[x],Y:[[y]],dropout:0})\n",
        "print(\"\\nguess on test:\",test_guess )\n",
        "print(\"error:\",math.sqrt(error))\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Survived                           0\n",
            "Pclass                             2\n",
            "Sex                                1\n",
            "Age                               21\n",
            "SibSp                              1\n",
            "Parch                              0\n",
            "Fare                            11.5\n",
            "Embarked                [1, 0, 0, 0]\n",
            "CabinA      [0, 0, 0, 0, 0, 0, 0, 0]\n",
            "CabinB                             0\n",
            "Name: 861, dtype: object\n",
            "\n",
            "guess on test: [[0.24798352 0.24769664 0.24814212 0.24796711 0.24784565 0.24808794\n",
            "  0.24775033 0.24747139 0.24786322 0.2479305  0.24796183 0.24808967\n",
            "  0.24799599 0.24830061 0.24785405 0.24752308 0.24780232 0.24833287\n",
            "  0.24760026 0.2478061  0.2475564  0.24815735 0.24825649 0.24737954\n",
            "  0.24776569 0.24822262 0.2478931  0.24820395 0.2476953  0.24812602\n",
            "  0.24808589 0.24855469 0.2478621  0.24802755 0.24837983 0.24803258\n",
            "  0.24793261 0.24785253 0.24804191 0.24811503 0.24836104 0.2481059\n",
            "  0.24848242 0.24795595 0.24815838 0.24802954 0.24807274 0.24778663\n",
            "  0.24820028 0.24772269 0.24766897 0.24832048 0.24762604 0.24791312\n",
            "  0.24791716 0.24758774 0.24791177 0.24852723 0.24830708 0.24788934\n",
            "  0.24795218 0.24781232 0.24801195 0.24766295]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-efa4388981d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nguess on test:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_guess\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9VexKUObToq",
        "colab_type": "code",
        "outputId": "6abeb3bb-932f-4426-8164-84ce140ffa78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1901
        }
      },
      "source": [
        "new=np.array([[0],[1],[1],[30],[0],[0],[30],[1,0,0,0],[1,0,0,0,0,0,0,0],[31]])\n",
        "x,y=prep(new)\n",
        "tracker=sess.run([out_layer,layers,X], feed_dict={X:[x], Y: [y]})\n",
        "test_guess = tracker[0]\n",
        "layer_values=tracker[1]\n",
        "inputs=tracker[2]\n",
        "print(test_guess[0][0])\n",
        "print(tracker[2])\n",
        "layer_values[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.24798352\n",
            "[[ 1.  1. 30.  0.  0. 30.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
            "  31.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 2.1990747 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.41410166, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 4.667697  ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 3.543169  , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 1.5066113 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        3.5985923 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02485857, 0.        , 3.5209105 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 1.8463856 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        2.3785565 , 0.8653297 , 2.2520955 , 4.183631  , 2.0204878 ,\n",
              "        0.        , 0.        , 0.        , 0.8845189 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 4.540491  , 0.        ,\n",
              "        3.2215173 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 3.6035357 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.77190274, 0.        , 0.        , 0.        ,\n",
              "        0.        , 1.4765455 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.8658534 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.46383736, 0.        ,\n",
              "        0.        , 3.5051873 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 9.501643  , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 3.7793343 , 0.        , 5.7801533 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.17927733, 0.        , 1.6055242 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.36027497,\n",
              "        0.        , 0.        , 5.4935474 , 0.48195323, 1.5502672 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 1.134083  ,\n",
              "        0.        , 0.        , 0.        , 6.6773176 , 0.        ,\n",
              "        0.        , 0.01392112, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.653674  , 0.68426394, 0.0441377 ,\n",
              "        0.        , 0.        , 1.407079  , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 1.3785242 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 2.774481  ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 2.330822  , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 2.6279352 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        5.432145  , 0.        , 0.20143853, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.08200023,\n",
              "        0.84135044, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 1.2103404 , 1.8550404 ,\n",
              "        0.46013385, 6.005546  , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 3.4884796 , 0.        , 3.1718776 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 5.258073  , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 4.588343  , 0.        , 0.        ,\n",
              "        2.3678274 , 0.        , 0.        , 3.6597185 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 1.5291069 , 0.7728376 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 6.4105864 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        2.5024521 , 2.4504862 , 0.        , 4.7261987 , 0.        ,\n",
              "        0.        , 6.660885  , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 1.3773726 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 3.1949658 , 1.3149626 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.759802  , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.01284477, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.21018258, 0.        ,\n",
              "        0.77866584, 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pvqw84HDHd4",
        "colab_type": "text"
      },
      "source": [
        "Kaggle test subission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBSRPwe5DDSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_url=\"https://raw.githubusercontent.com/brianbob12/brianbob12.github.io/master/titanic_test.csv\"\n",
        "test_data=pd.read_csv(test_url)\n",
        "test_data=test_data.drop(columns=[\"Name\",\"Ticket\"])#remove irrelecent or unusable data\n",
        "\n",
        "#split the cabin data into two poits\n",
        "pre_CabinA=[]#CabinA is the cabin letter(as an array of 8  -Nan and the 7 letters)\n",
        "#pre_CabinA    [A,B,C,D,E,F,G,T,nan]\n",
        "pre_CabinB=[]#CabinB is the cabin number 0 for nan\n",
        "for cab in test_data[\"Cabin\"]:\n",
        "  if str(cab) ==\"nan\":\n",
        "    pre_CabinA.append(np.array([0 for i in range(8)]))\n",
        "    pre_CabinB.append(0)\n",
        "  elif len(str(cab).split(\" \"))>1:\n",
        "    a=str(cab).split(\" \")\n",
        "    for pos in a:\n",
        "      if len(str(pos))>1:\n",
        "        new_cab=pos\n",
        "    \n",
        "    let=new_cab[0]\n",
        "    to_let=[0 for i in range(8)]\n",
        "    if let!=\"T\":\n",
        "      to_let[ord(let)-65]=1\n",
        "    else:\n",
        "      to_let[6]=1\n",
        "    num=int(new_cab[1:])\n",
        "    pre_CabinA.append(np.array(to_let))\n",
        "    pre_CabinB.append(num)\n",
        "  else:\n",
        "    \n",
        "    if len(str(cab))==1:\n",
        "      new_cab=cab+\"0\"\n",
        "    else:\n",
        "      new_cab=cab\n",
        "    \n",
        "    let=new_cab[0]\n",
        "    to_let=[0 for i in range(8)]\n",
        "    if let!=\"T\":\n",
        "      to_let[ord(let)-65]=1\n",
        "    else:\n",
        "      to_let[6]=1\n",
        "    num=int(new_cab[1:])\n",
        "    pre_CabinA.append(np.array(to_let))\n",
        "    pre_CabinB.append(num)\n",
        "test_data[\"CabinA\"]=pre_CabinA\n",
        "test_data[\"CabinB\"]=pre_CabinB\n",
        "new_age=[]\n",
        "for ind in test_data[\"Age\"]:\n",
        "  if str(ind).lower()==\"nan\":\n",
        "    new_age.append(-1)#unavailable age\n",
        "  else:\n",
        "    new_age.append(int(ind))\n",
        "test_data[\"Age\"]=new_age\n",
        "test_data=test_data.drop(columns=[\"Cabin\"])\n",
        "\n",
        "#embarked\n",
        "#array of 4\n",
        "new_embark=[]\n",
        "un=test_data[\"Embarked\"].unique()\n",
        "for i in test_data[\"Embarked\"]:\n",
        "  for inx,j in enumerate(un):\n",
        "    if str(i)==str(j):\n",
        "      ps=[0 for i in range(4)]\n",
        "      ps[inx]=1\n",
        "      new_embark.append(np.array(ps))\n",
        "      break\n",
        "test_data[\"Embarked\"]=new_embark\n",
        "#sex\n",
        "#0-female\n",
        "#1-male\n",
        "new_sex=[]\n",
        "for i in test_data[\"Sex\"]:\n",
        "  if i==\"female\":\n",
        "    new_sex.append(0)\n",
        "  else:\n",
        "    new_sex.append(1)\n",
        "test_data[\"Sex\"]=new_sex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEMC2ZESEWMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out=pd.DataFrame({\"PassengerId\":[],\"Survived\":[]})\n",
        "passengar=[]\n",
        "life=[]\n",
        "for i in range(test_data.count()[0]):\n",
        "  x,y=prep(test_data.loc[i])#y is passengarId\n",
        "  passengar.append(y)\n",
        "  life.append(round(sess.run(out_layer,feed_dict={X:[x],Y:[[y]]})[0][0]))\n",
        "out[\"PassengerId\"]=passengar\n",
        "out[\"Survived\"]=life\n",
        "out.to_csv(\"preds.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}